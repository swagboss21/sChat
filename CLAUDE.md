# Multi-Model AI Research Tool - Development Log

## Project Overview
A FastAPI-based web application that queries multiple AI models in parallel via OpenRouter, then synthesizes their responses into actionable intelligence. All models have real-time web search enabled.

---

## Current Configuration (as of Nov 11, 2025)

### Models (5 Initial Prompt Models)
All models use `:online` suffix for real-time web search:

1. **openai/gpt-4o:online** - OpenAI's flagship model with web search
2. **anthropic/claude-sonnet-4.5:online** - Best reasoning model with web search
3. **perplexity/sonar-pro** - Native web search and citations (already has built-in search)
4. **x-ai/grok-4:online** - xAI's Grok with web + X/Twitter search
5. **meta-llama/llama-3.1-70b-instruct:online** - Open source model with web search

### Aggregator Models (6 Options)
All available in dropdown with web search enabled:
- anthropic/claude-sonnet-4.5:online (default)
- openai/gpt-4o:online
- perplexity/sonar-pro
- x-ai/grok-4:online
- meta-llama/llama-3.1-70b-instruct:online
- deepseek/deepseek-chat:online (synthesis only, not in initial prompts)

### Token Limits
- **Individual models**: 4,000 max_tokens
- **Aggregator**: 5,000 max_tokens
- **Timeout**: 60 seconds per model (120 seconds for Grok due to X/Twitter integration)

### Cost Estimate
- **~$0.50-0.80 per request** (5 models + aggregation)
- **Well under $1/request target**

---

## Architecture

### Backend (main.py)
- FastAPI with async/await for parallel model calls
- Single POST endpoint: `/api/chat`
- Request format: `{prompt, models[], aggregator}`
- Response format: `{aggregated, individual[]}`
- Error handling: Continues with successful responses if some models fail
- OpenRouter client with base_url: `https://openrouter.ai/api/v1`

### Frontend (static/index.html)
- Vanilla HTML/CSS/JavaScript (no frameworks)
- Dark theme UI
- Model selector: 5 checkboxes (all checked by default)
- Aggregator selector: dropdown with 6 options
- Real-time loading states
- Collapsible individual responses section
- Web search indicator: üåê icon on all models

### Aggregation Prompt
```
Synthesize these AI model responses into actionable intelligence.

Original Query: {query}

Model Responses:
{formatted_responses}

Provide a structured synthesis:

## KEY CONSENSUS
What most/all models agree on

## UNIQUE INSIGHTS
Novel information found by only 1-2 models (prioritize data sources, links, tweets)

## CONTRADICTIONS
Where models disagree or conflict

## ACTIONABLE INTELLIGENCE
Concrete takeaways for decision-making

Be concise. Highlight sources and data.
```

---

## Testing Results

### Test 1: Initial Model Verification (test_models.py)
**Date**: Nov 11, 2025

**Tested Models**:
- ‚úÖ anthropic/claude-sonnet-4.5 - SUCCESS
- ‚úÖ openai/gpt-4o - SUCCESS
- ‚úÖ perplexity/sonar-pro - SUCCESS
- ‚úÖ deepseek/deepseek-chat - SUCCESS
- ‚ùå google/gemini-2.0-flash-exp - FAILED (404 - endpoint not found)

**Result**: 4/5 models working

---

### Test 2: Grok Model ID Verification (test_grok.py)
**Date**: Nov 11, 2025

**Tested Models**:
- ‚úÖ x-ai/grok-4 - SUCCESS
- ‚úÖ x-ai/grok-4-fast - SUCCESS

**Note**: Original `x-ai/grok-beta` returned 404. Correct IDs are `grok-4` or `grok-4-fast`.

---

### Test 3: Web Search Verification (test_web_search.py)
**Date**: Nov 11, 2025

**Model**: openai/gpt-4o:online

**Query**: "What is today's date and what is the top news story today?"

**Result**: ‚úÖ SUCCESS
- Returned correct date: **November 11, 2025**
- Retrieved current news about government shutdown and Trump pardons
- Included source citation: nbcnews.com
- Confirms `:online` suffix enables real-time web search

---

### Test 4: Full System Test - Government Shutdown Query
**Date**: Nov 11, 2025

**Query**:
```
What is the current status of the US government shutdown as of today? Include:
1. How many days has it been ongoing?
2. What are the main sticking points preventing a deal?
3. What recent votes or developments happened in Congress?
4. Which federal services are most affected?
5. What do experts predict will happen next?

Provide specific dates, names, and cite your sources.
```

**Results**:

| Model | Status | Tokens | Notes |
|-------|--------|--------|-------|
| gpt-4o:online | ‚ùå Failed | 0 | 402 - Insufficient credits |
| claude-sonnet-4.5:online | ‚ùå Failed | 0 | 402 - Insufficient credits |
| perplexity/sonar-pro | ‚ùå Failed | 0 | 402 - Insufficient credits |
| x-ai/grok-4:online | ‚ùå Failed | 0 | 402 - Insufficient credits |
| llama-3.1-70b-instruct:online | ‚úÖ Success | 2,672 | Full response with citations |
| Aggregator | ‚ùå Failed | 0 | 402 - Insufficient credits |

**Llama 3.1 Response Highlights**:
- ‚úÖ Found shutdown duration: **37+ days** (as of Nov 6, 2025)
- ‚úÖ Identified as **longest in US history**
- ‚úÖ Cited impact on **flight cancellations/delays** (FAA)
- ‚úÖ Mentioned **Senate stopgap deal** in progress
- ‚úÖ Multiple sources: CBS News, BBC, Fox News, UpNorthLive
- ‚úÖ Current November 2025 information confirmed

**Key Finding**: Web search is fully functional and retrieving current information. Credit limit prevented full 5-model test.

---

### Test 5: Initial Simple Query (User Test)
**Date**: Nov 11, 2025

**Query**: "top news stories today about the government shutdown"

**Results**:

| Model | Status | Tokens | Key Info |
|-------|--------|--------|----------|
| gpt-4o:online | ‚úÖ Success | 2,247 | Senate 60-40 vote, 41-day shutdown, bill advances |
| claude-sonnet-4.5:online | ‚ùå Failed | 0 | 402 - Insufficient credits |
| perplexity/sonar-pro | ‚ùå Failed | 0 | 402 - Insufficient credits |
| x-ai/grok-4:online | ‚ùå Failed | 0 | 402 - Insufficient credits |
| llama-3.1-70b-instruct:online | ‚úÖ Success | 2,198 | Senate passes funding bill, 41 days, multiple sources |
| Aggregator | ‚ùå Failed | 0 | 402 - Insufficient credits |

**Successful Responses**:
- Both models returned **November 2025 dates** (correct)
- Cited sources: CBS News, ABC News, Reuters, USA Today, White House
- Found current information about **41-day shutdown**
- Mentioned **Senate procedural vote (60-40)**
- Referenced **House vote expected Wednesday**

**Key Finding**: Models with sufficient credits successfully searched web and returned current, cited information.

---

## Known Issues

### Issue 1: OpenRouter Credit Limit
**Status**: Blocking full testing
**Description**: User's OpenRouter API key has insufficient credits (~726 remaining)
**Impact**: Most models fail with 402 error when requesting 4,000 tokens
**Solution Required**: Add $5-10 credits at https://openrouter.ai/settings/keys
**Workaround**: Temporarily reduce token limits (not recommended - reduces quality)

### Issue 2: Grok 4 Timeout (Unconfirmed)
**Status**: Needs investigation
**Description**: User reported Grok timed out during successful test, while other models completed
**Current Timeout**: 60 seconds
**Hypothesis**: Grok's web search may take longer, especially with X/Twitter integration
**Next Steps**:
- Monitor Grok response times across multiple queries
- Consider increasing timeout to 90-120 seconds if pattern confirmed
- May be credit-related (402 error) rather than actual timeout

### Issue 3: Date Context Issue (Resolved)
**Status**: ‚úÖ Resolved
**Description**: Models initially returned October 2023 dates instead of November 2025
**Root Cause**: Models without `:online` suffix use static training data cutoffs
- GPT-4o: October 2023 cutoff
- Claude Sonnet 4.5: April 2024 cutoff
**Solution**: Added `:online` suffix to all models to enable real-time web search
**Verification**: Confirmed working - models now return November 2025 dates

---

## File Structure
```
/chatbot/
‚îú‚îÄ‚îÄ main.py                          # FastAPI backend
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ index.html                   # Frontend UI
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ .env                            # OpenRouter API key
‚îú‚îÄ‚îÄ test_models.py                  # Initial model testing
‚îú‚îÄ‚îÄ test_new_models.py              # Grok + 5th model search
‚îú‚îÄ‚îÄ test_grok.py                    # Grok ID verification
‚îú‚îÄ‚îÄ test_web_search.py              # Web search verification
‚îî‚îÄ‚îÄ CLAUDE.md                       # This file
```

---

## Dependencies (requirements.txt)
```
fastapi==0.115.5
uvicorn==0.32.1
openai==1.55.3
python-dotenv==1.0.1
```

---

## Environment Variables (.env)
```
OPENROUTER_API_KEY=sk-or-v1-[your-key-here]
```

---

## How to Run
```bash
# Install dependencies
pip install -r requirements.txt

# Start server
uvicorn main:app --reload

# Visit in browser
http://localhost:8000
```

---

## Design Decisions

### Why `:online` Suffix?
- Simple, declarative approach to enable web search
- Works with any OpenRouter model
- Alternative to plugin API syntax (cleaner)
- Perplexity models have built-in search, don't need suffix but it doesn't hurt

### Why DeepSeek in Aggregator Only?
- User request: Remove from initial prompts
- Keep available for synthesis (very cost-effective at $0.30/M tokens)
- Allows testing different aggregation perspectives

### Why 5 Models?
- User requested 5 for diverse perspectives
- Mix of providers: OpenAI, Anthropic, Perplexity, xAI, Meta
- Mix of capabilities: reasoning (Claude, GPT), search (Perplexity), X integration (Grok), open-source (Llama)

### Why High Token Limits?
- User priority: Quality over cost during testing phase
- 4K tokens allows comprehensive responses with sources
- 5K aggregation allows full synthesis with all sections
- Still under $1/request target

---

## Next Steps

### Immediate (User Requested)
1. ‚úÖ Document current state in CLAUDE.md
2. üîÑ Review internal prompts for optimization
3. üîÑ Run additional testing with various query types
4. üîÑ Monitor Grok timeout behavior (may need 90-120 second timeout)

### Future Improvements
- Add configurable token limits in UI
- Add cost estimator per request
- Add chat history/persistence (explicitly not in MVP)
- Add model response time tracking
- Consider adding reasoning models (o3-mini, DeepSeek R1, QwQ-32B)
- Add Perplexity Sonar Reasoning Pro for web + reasoning combo

---

## Success Metrics

### ‚úÖ Working Features
- Parallel model queries via OpenRouter
- Real-time web search on all models
- Current information retrieval (November 2025)
- Source citations in responses
- Aggregation synthesis with structured output
- Error handling (continues with partial success)
- Clean UI with loading states
- Collapsible individual responses
- Customizable aggregator model

### üîÑ Needs More Testing
- Grok 4 response times and timeout tolerance
- Different query types (news, research, technical)
- Cost tracking across different queries
- Aggregator quality comparison (which model synthesizes best?)

### ‚ùå Blocked by Credits
- Full 5-model parallel testing
- Aggregation synthesis testing
- Cost-per-request validation
- Performance benchmarking

---

## Research Notes

### OpenRouter Model Research Findings
From Context7 research on 2025-11-11:

**Models with Native Web Search**:
- Perplexity Sonar series (all variants)
- openai/gpt-4o-search-preview (specialized)

**Web Search via `:online` Suffix**:
- Works with ANY model on OpenRouter
- Adds ~$0.02-0.05 per request for search costs
- Uses Exa or native search engines

**Reasoning Models** (for future consideration):
- openai/o3-mini: $1.10/M input, supports reasoning_effort parameter
- deepseek/deepseek-r1: $0.30/M input, excellent value
- qwen/qwq-32b: $0.05/M input, ultra-cheap

**Best for Research** (future consideration):
- perplexity/sonar-reasoning-pro: Web search + reasoning combined
- perplexity/sonar-deep-research: Autonomous multi-step research

---

## Cost Analysis

### Current Setup (5 models + aggregator)
**Per Request Estimate**:
- GPT-4o:online: ~$0.060 (4K tokens + search)
- Claude Sonnet 4.5:online: ~$0.071 (4K tokens + search)
- Perplexity Sonar Pro: ~$0.041 (4K tokens, built-in search)
- Grok 4:online: ~$0.055 (4K tokens + search)
- Llama 3.1:online: ~$0.038 (4K tokens + search)
- Aggregator (Claude 5K): ~$0.054

**Total**: ~$0.32/request ‚úÖ Under $1 target

**Budget Planning**:
- $10 credits = ~31 requests
- $25 credits = ~78 requests
- $50 credits = ~156 requests

---

## Changelog

### 2025-11-11 (Initial Build)
- ‚úÖ Created project structure (main.py, index.html, requirements.txt)
- ‚úÖ Implemented parallel model queries with asyncio
- ‚úÖ Added aggregation synthesis with structured prompt
- ‚úÖ Enabled web search via `:online` suffix on all models
- ‚úÖ Replaced DeepSeek with Grok 4 in initial models
- ‚úÖ Added Llama 3.1 70B as 5th model
- ‚úÖ Kept DeepSeek in aggregator options only
- ‚úÖ Increased token limits: 4K individual, 5K aggregator
- ‚úÖ Added web search indicators (üåê) in UI
- ‚úÖ Verified web search functionality via testing
- ‚úÖ Confirmed current date retrieval (November 2025)
- ‚úÖ Documented all findings in CLAUDE.md

### 2025-11-11 (Optimization Pass)
**Analysis of Government Shutdown Test Query Results:**
- ‚úÖ Fixed token count display bug (using `completion_tokens` instead of inflated `total_tokens`)
- ‚úÖ Increased Grok timeout from 60s to 120s (X/Twitter integration requires more time)
- ‚úÖ Enhanced aggregation prompt with deduplication instructions
- ‚úÖ Added model capability context hints to aggregator (e.g., "strong reasoning", "X/Twitter access")
- ‚úÖ Clarified "UNIQUE INSIGHTS" definition (only 1-2 models, not repeated info)
- üìä Identified Claude Sonnet 4.5 as highest-value model (80% of unique insights)
- üìä Identified GPT-4o as lowest-value model (generic responses, no unique insights)
- üìä Confirmed web search working across all successful models (November 2025 data retrieved)

### 2025-11-11 (Prompt Optimization Feature)
**Added AI-powered prompt optimization:**
- ‚úÖ New `/api/optimize` endpoint using Claude 3.5 Haiku (no :online - uses system date)
- ‚úÖ Frontend "üîß Optimize Prompt" button with smart state management
- ‚úÖ One-click revert with "‚èÆ Back to Original" functionality
- ‚úÖ Automatic state reset when user manually edits optimized prompt
- ‚úÖ Prevents repeated optimization (token waste protection)
- ‚úÖ Cost: ~$0.003-0.005 per optimization (under half a cent)
- ‚úÖ **System date injection**: Uses `datetime.now()` for accurate current date (not web search)
  - Current events: Adds "as of November 10, 2025" (system date)
  - Historical queries: Keeps original timeframe (e.g., "2008 crisis", "WWII")
  - Model analyzes keywords to determine current vs. historical
- ‚úÖ **Output cleaning**: Removes meta-commentary and explanatory preambles
- ‚úÖ **Query logging**: All optimizations logged to `optimization_log.json`
- üéØ Optimization principles: Fix typos, add structure, increase specificity, demand sources, guide output format

### 2025-11-11 (Developer Logging)
**Added comprehensive logging for testing/debugging:**
- ‚úÖ **optimization_log.json**: Logs every optimization attempt with timestamp, original, optimized, model, tokens
- ‚úÖ **query_log.json**: Logs every full research query with:
  - User prompt
  - All 5 model responses (individual text, tokens, errors)
  - Aggregated synthesis response
  - Total token count
  - Timestamp
- üéØ Purpose: Compare test results, track model performance, analyze optimization quality over time

---

## Optimization Analysis & Recommendations

### Summary of Nov 11 Test Results

**Query**: Government shutdown status with 5 detailed sub-questions
**Models Tested**: GPT-4o, Claude Sonnet 4.5, Perplexity Sonar Pro, Grok 4, Llama 3.1 70B

| Model | Success | Tokens Out | Quality Grade | Unique Value |
|-------|---------|------------|---------------|--------------|
| Claude Sonnet 4.5 | ‚úÖ | ~2,700 | A+ | 80% of unique insights (ACA details, negotiator names, GDP loss, sentiment data) |
| Perplexity Sonar Pro | ‚úÖ | ~780 | B+ | Good citations, concise, border security mention |
| Llama 3.1 70B | ‚úÖ | ~2,500 | B | White House shutdown clock URL, multiple sources |
| GPT-4o | ‚úÖ | ~2,670 | C | Generic overview, no unique insights, basic facts only |
| Grok 4 | ‚ùå | 0 | N/A | Timeout (2/2 attempts) - now fixed with 120s timeout |
| **Aggregator** (Claude) | ‚úÖ | ~5,000 | A | Excellent synthesis with clear sections |

### Key Findings

1. **Claude Dominates Research Quality**
   - Provided 80% of the "UNIQUE INSIGHTS" section content
   - Only model that found: ACA subsidies as main sticking point, specific negotiator names, $7B GDP loss estimate, public sentiment shift data (36% vs 21%)
   - Most detailed, sourced responses

2. **GPT-4o Underperformed**
   - Generic summaries that added no unique value
   - Similar token usage to Claude (~2,700) but far less information density
   - Cost-ineffective for research queries

3. **Grok Needs Special Handling**
   - Confirmed timeout on 2/2 attempts at 60 seconds
   - X/Twitter search integration likely requires more time
   - **Fix Applied**: Increased timeout to 120 seconds for Grok models

4. **Perplexity Performed Well**
   - Most concise responses (~780 tokens vs 2,500-2,700 for others)
   - Strong citations despite being 3-4x shorter
   - Cost-effective for web search tasks

5. **Token Count Bug Discovered**
   - Claude showed 76,460 tokens (impossible with 4,000 max_tokens limit)
   - OpenRouter API returning inflated `total_tokens` values
   - **Fix Applied**: Now using `completion_tokens` with fallback validation

### Implemented Optimizations

#### 1. Fixed Token Counting (main.py:76-83)
```python
# Get token count with validation (OpenRouter sometimes returns inflated values)
tokens = 0
if response.usage:
    # Use completion_tokens for actual output size
    tokens = response.usage.completion_tokens
    # Fallback to total_tokens if completion not available, but cap at max_tokens
    if not tokens:
        tokens = min(response.usage.total_tokens, 4000)
```

#### 2. Dynamic Timeout by Model (main.py:65-66)
```python
# Grok models need more time for X/Twitter search integration
timeout = 120.0 if "grok" in model.lower() else 60.0
```

#### 3. Enhanced Aggregation Prompt (main.py:130-149)
**Changes:**
- Added "3+ models" threshold for KEY CONSENSUS
- Explicit deduplication rule: "Do not repeat the same facts across multiple sections"
- Clarified UNIQUE INSIGHTS as "ONLY 1-2 models"
- Added X/Twitter prioritization for Grok data
- Emphasized specificity (dates, names, numbers)
- Structured ACTIONABLE INTELLIGENCE with bullets

#### 4. Model Capability Context (main.py:114-128)
Added context hints to help aggregator understand model strengths:
```python
model_notes = {
    "anthropic/claude-sonnet-4.5:online": " (strong reasoning, detailed analysis)",
    "x-ai/grok-4:online": " (real-time X/Twitter access)",
    "perplexity/sonar-pro": " (native web search with citations)"
}
```

### Future Optimization Considerations

#### Option A: Cost Optimization - Reduce Model Count
**Rationale**: If 80% of value comes from Claude, why pay for 5 models?

**Proposed 3-Model Setup** (~$0.20/request):
1. **Claude Sonnet 4.5:online** - Primary research (high detail)
2. **Perplexity Sonar Pro** - Fast search with citations (cost-effective)
3. **Grok 4:online** - X/Twitter real-time data (unique source)

**Dropped**: GPT-4o (generic), Llama 3.1 (overlaps with others)

**Pros**: 60% cost reduction, faster responses, less redundancy
**Cons**: Less model diversity, fewer perspectives

#### Option B: Quality Optimization - Add Reasoning Models
**Rationale**: Current models search well but don't deeply analyze

**Enhanced 6-Model Setup** (~$0.60/request):
- Keep current 5 models
- Add **perplexity/sonar-reasoning-pro** (web search + deep reasoning)
- Use reasoning model for complex analytical queries only

#### Option C: Hybrid Approach - Dynamic Model Selection
**Rationale**: Different queries need different model combinations

**Implementation**: Add query type detection
- **News/Current Events**: Claude + Perplexity + Grok (3 models)
- **Technical Research**: Claude + GPT-4o + DeepSeek R1 (3 models)
- **Social Media Monitoring**: Grok + Perplexity + Llama (3 models)
- **Deep Analysis**: Add reasoning models to any combination

**Pros**: Optimized cost per query, best model fit
**Cons**: Complex logic, requires query classification

### Recommended Next Steps

1. **Immediate** (Already Implemented ‚úÖ)
   - Fix token count display bug
   - Increase Grok timeout to 120s
   - Improve aggregation prompt deduplication

2. **Short Term** (Test Next)
   - Run 5-10 diverse queries to validate optimizations
   - Compare aggregation quality before/after prompt changes
   - Confirm Grok now completes successfully with 120s timeout
   - Track which models consistently provide unique value

3. **Medium Term** (Consider After Data Collection)
   - Decision: Keep 5 models or reduce to 3-model setup?
   - A/B test different aggregator models (try GPT-4o, Perplexity)
   - Add cost tracking to UI (show $ per query)
   - Consider adding reasoning models for complex queries

4. **Long Term** (Future Features)
   - Query type detection for dynamic model selection
   - Model performance scoring/ranking system
   - User feedback loop ("Was this response valuable?")
   - Response caching for repeated queries

---

## Contact & Credits

**Built with**: Claude Code (Anthropic)
**OpenRouter API**: https://openrouter.ai
**Models Used**: OpenAI, Anthropic, Perplexity, xAI, Meta

---

*Last Updated: November 11, 2025*
