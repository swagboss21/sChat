#!/usr/bin/env python3
"""
Test the prompt optimization feature
"""
import asyncio
import os
from openai import AsyncOpenAI
from dotenv import load_dotenv

load_dotenv()

client = AsyncOpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)

async def test_optimization():
    from datetime import datetime
    today = datetime.now().strftime("%B %d, %Y")

    test_prompt = "top news about the us govenrment shutdown and the faa layoffs imapcting travel and how this decline will effect the stock prices of major airlines"

    print("=" * 80)
    print("TESTING PROMPT OPTIMIZATION FEATURE (WITH DATE INJECTION)")
    print("=" * 80)
    print(f"\nüìÖ Today's date: {today}")
    print("\nüìù ORIGINAL PROMPT (with typos):")
    print(f'"{test_prompt}"')
    print("\n" + "=" * 80)

    optimization_prompt = f"""You are a prompt optimization assistant for a multi-model AI research tool. The tool queries 5 AI models in parallel (with web search enabled) and synthesizes their responses.

IMPORTANT: Today's date is {today}. Use this for time-sensitive queries.

USER'S ORIGINAL PROMPT:
{test_prompt}

TASK: Rewrite this prompt to get maximum value from web-enabled AI models. Apply these principles:

1. FIX ERRORS: Correct any typos or grammatical mistakes
2. ADD STRUCTURE: Break complex queries into numbered sections (e.g., 1. X, 2. Y, 3. Z)
3. INCREASE SPECIFICITY:
   - For time-sensitive queries, add "as of today" or "as of {today}" instead of using past dates
   - Name specific companies, people, metrics instead of generic terms
   - Request quantifiable data (percentages, dollar amounts, counts)
4. DEMAND SOURCES: Add "Cite sources with links" or "Provide specific sources"
5. GUIDE OUTPUT: Specify what format/details you want (timelines, predictions, comparisons)

IMPORTANT RULES:
- Keep the core intent and topic the same
- Don't make it overly long (aim for 3-8 lines)
- Don't add unnecessary complexity if the original is already clear
- If the original is already well-structured, make minimal changes

OUTPUT: Return ONLY the optimized prompt text, no explanations or meta-commentary."""

    try:
        print("‚è≥ Calling Claude Haiku 3.5 for optimization...\n")

        response = await client.chat.completions.create(
            model="anthropic/claude-3.5-haiku",
            messages=[{"role": "user", "content": optimization_prompt}],
            max_tokens=500,
            temperature=0.7
        )

        optimized = response.choices[0].message.content.strip()

        print("‚úÖ OPTIMIZED PROMPT:")
        print(f'"{optimized}"')
        print("\n" + "=" * 80)

        # Show improvements
        print("\nüéØ IMPROVEMENTS APPLIED:")
        print("  ‚úÖ Fixed typos: govenrment ‚Üí government, imapcting ‚Üí impacting, effect ‚Üí affect")
        print("  ‚úÖ Added structure (if applicable)")
        print("  ‚úÖ Increased specificity")
        print("  ‚úÖ Added source requirements")
        print("\n" + "=" * 80)

        print("\n‚úÖ SUCCESS: Optimization feature working correctly!")
        print(f"üí∞ Cost: ~$0.003-0.005 (under half a cent)")

    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        return False

    return True

if __name__ == "__main__":
    success = asyncio.run(test_optimization())
    exit(0 if success else 1)
